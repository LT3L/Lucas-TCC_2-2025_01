import polars as pl
import argparse
import os
import glob
from pathlib import Path

# Argumentos de linha de comando
parser = argparse.ArgumentParser(description="Script Polars NYC")
parser.add_argument('--input', type=str, required=True, help='Caminho para o diretÃ³rio de entrada')
args = parser.parse_args()

# Lista todos os arquivos no diretÃ³rio
arquivos = sorted(glob.glob(os.path.join(args.input, "*.*")))
if not arquivos:
    print(f"âŒ Nenhum arquivo encontrado em {args.input}")
    exit(1)

# Verifica o formato do primeiro arquivo
primeiro_arquivo = arquivos[0]
extensao = Path(primeiro_arquivo).suffix.lower()

print(f"ğŸ“ Processando arquivos {extensao} do diretÃ³rio: {args.input}")
print(f"ğŸ“„ Arquivos encontrados: {len(arquivos)}")

# LÃª todos os arquivos de uma vez
if extensao == '.parquet':
    # Parquet jÃ¡ tem os campos de datetime no formato correto
    df = pl.concat([pl.read_parquet(f) for f in arquivos])
elif extensao in ['.json', '.jsonl']:
    # Para JSON, os timestamps estÃ£o em formato Unix
    df = pl.concat([pl.read_ndjson(f) for f in arquivos])
    # Converte timestamps Unix para datetime
    df = df.with_columns([
        pl.from_epoch(pl.col("tpep_pickup_datetime").cast(pl.Int64)),
        pl.from_epoch(pl.col("tpep_dropoff_datetime").cast(pl.Int64))
    ])
elif extensao == '.csv':
    # Primeiro, lÃª um arquivo para inferir o schema
    schema = pl.read_csv(arquivos[0], try_parse_dates=True).schema
    # LÃª todos os CSVs usando o mesmo schema
    df = pl.concat([pl.read_csv(f, schema=schema) for f in arquivos])
    # NÃ£o precisa converter datetime pois jÃ¡ estÃ¡ no formato correto
else:
    print(f"âŒ Formato nÃ£o suportado para o arquivo: {primeiro_arquivo}")
    exit(1)

print(f"ğŸ“Š Total de registros lidos: {df.height}")

# Remove apenas linhas onde os campos essenciais sÃ£o nulos
campos_essenciais = ["tpep_pickup_datetime", "tpep_dropoff_datetime", "passenger_count", "trip_distance"]
df = df.drop_nulls(subset=campos_essenciais)
print(f"ğŸ“Š Registros apÃ³s remover nulos essenciais: {df.height}")

# Criar coluna de duraÃ§Ã£o da viagem (minutos)
df = df.with_columns(
    ((pl.col("tpep_dropoff_datetime").cast(pl.Int64) - pl.col("tpep_pickup_datetime").cast(pl.Int64)) / 60).alias("trip_duration")
)

# Adicionar colunas derivadas
df = df.with_columns(
    (pl.col("trip_distance") / (pl.col("trip_duration") / 60)).alias("average_speed_kmh"),
    (pl.col("fare_amount") / pl.col("trip_distance")).alias("fare_per_km"),
    pl.col("tpep_pickup_datetime").dt.year().alias("year"),
    pl.col("tpep_pickup_datetime").dt.month().alias("month"),
    pl.col("tpep_pickup_datetime").dt.day().alias("day"),
    pl.col("tpep_pickup_datetime").dt.hour().alias("hour")
)

# Filtrar viagens invÃ¡lidas
df = df.filter(
    (pl.col("trip_duration") > 0) &
    (pl.col("passenger_count") > 0) &
    (pl.col("trip_distance") > 0)
)
print(f"ğŸ“Š Registros apÃ³s filtrar viagens invÃ¡lidas: {df.height}")

# Criar pasta se nÃ£o existir
output_folder = "processed_data/"
os.makedirs(output_folder, exist_ok=True)

# Salvar resultados
output_file = os.path.join(output_folder, "nyc_taxi_processed.parquet")
df.write_parquet(output_file)
print(f"âœ… Dados processados salvos em: {output_file}")

# Exibir estatÃ­sticas bÃ¡sicas
if df.height > 0:
    print("\nğŸ“ˆ EstatÃ­sticas bÃ¡sicas:")
    print(f"Total de viagens: {df.height}")
    print(f"PerÃ­odo: {df['year'].min()} a {df['year'].max()}")
    print(f"DistÃ¢ncia mÃ©dia: {df['trip_distance'].mean():.2f} km")
    print(f"Valor mÃ©dio: ${df['fare_amount'].mean():.2f}")
else:
    print("\nâŒ Nenhum dado vÃ¡lido apÃ³s processamento")